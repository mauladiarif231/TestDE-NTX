{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in c:\\users\\metranet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.5.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\metranet\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Qlik.Sense.HTTP.Tunneling.Vulnerability\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML snippet representing the content\n",
    "html_snippet = '''\n",
    "<div class=\"col-lg\" style=\"word-break:break-all\">\n",
    "    <small><img class=\"me-1\" src=\"https://filestore.fortinet.com/fortiguard/static/images/icons_white/ips.svg?v=24687\" width=\"20\" alt=\"ips-logo\"> Intrusion Prevention</small>\n",
    "    <br>\n",
    "    <b>Qlik.Sense.HTTP.Tunneling.Vulnerability</b>\n",
    "</div>\n",
    "'''\n",
    "\n",
    "# Parse the HTML snippet\n",
    "soup = BeautifulSoup(html_snippet, 'html.parser')\n",
    "\n",
    "# Extract the title\n",
    "title_element = soup.find('b')\n",
    "title = title_element.text.strip() if title_element else 'Title not found'\n",
    "\n",
    "print('Title:', title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Level 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to CSV for level 1\n",
      "Scraping Level 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to CSV for level 2\n",
      "Scraping Level 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to CSV for level 3\n",
      "Scraping Level 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to CSV for level 4\n",
      "Scraping Level 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to CSV for level 5\n",
      "Skipped pages saved to JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Menggunakan nest_asyncio untuk menjalankan asyncio di Jupyter Notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Definisikan URL dasar\n",
    "BASE_URL = \"https://www.fortiguard.com/encyclopedia?type=ips&risk={}&page={}\"\n",
    "\n",
    "# Jumlah halaman maksimum untuk setiap level\n",
    "max_pages = [10, 10, 10, 10, 10]\n",
    "\n",
    "# Direktori output\n",
    "OUTPUT_DIR = \"datasets\"\n",
    "\n",
    "# Fungsi untuk mengambil data dari URL\n",
    "async def scrape_data(url):\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        try:\n",
    "            response = await client.get(url)\n",
    "            response.raise_for_status()\n",
    "            return response.content\n",
    "        except (httpx.HTTPStatusError, httpx.RequestError) as exc:\n",
    "            print(f\"Error fetching URL: {url}\")\n",
    "            print(exc)\n",
    "            return None\n",
    "\n",
    "def parse_html(html):\n",
    "    if html is None:\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    data = []\n",
    "    for item in soup.find_all(\"section\", class_=\"table-body\"):\n",
    "        title_element = item.find(\"b\")\n",
    "        if title_element:\n",
    "            title = title_element.text.strip()\n",
    "            link_element = item.find(\"div\", onclick=True)\n",
    "            if link_element and 'onclick' in link_element.attrs:\n",
    "                link = 'https://www.fortiguard.com' + link_element['onclick'].split(\"'\")[1]\n",
    "                data.append({\"title\": title, \"link\": link})\n",
    "    return data\n",
    "\n",
    "# Fungsi untuk menyimpan data ke dalam file CSV\n",
    "def save_to_csv(level, data):\n",
    "    filename = os.path.join(OUTPUT_DIR, f\"forti_lists_{level}.csv\")\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"Title,Link\\n\")\n",
    "        for item in data:\n",
    "            f.write(f\"{item['title']},{item['link']}\\n\")\n",
    "    print(f\"Data written to CSV for level {level}\")\n",
    "\n",
    "# Fungsi untuk menyimpan halaman yang dilewati ke dalam file JSON\n",
    "def save_skipped_pages(skipped):\n",
    "    with open(os.path.join(OUTPUT_DIR, \"skipped.json\"), \"w\") as f:\n",
    "        json.dump(skipped, f, indent=4)\n",
    "    print(\"Skipped pages saved to JSON\")\n",
    "\n",
    "# Fungsi utama untuk pengikisan data\n",
    "async def main():\n",
    "    skipped_pages = {}\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    for level, max_page in enumerate(max_pages, start=1):\n",
    "        print(f\"Scraping Level {level}...\")\n",
    "        level_data = []\n",
    "\n",
    "        for page in tqdm(range(1, max_page + 1)):\n",
    "            url = BASE_URL.format(level, page)\n",
    "            html = await scrape_data(url)\n",
    "\n",
    "            if html is None:\n",
    "                skipped_pages.setdefault(level, []).append(page)\n",
    "                continue\n",
    "\n",
    "            page_data = parse_html(html)\n",
    "            level_data.extend(page_data)\n",
    "\n",
    "        save_to_csv(level, level_data)\n",
    "\n",
    "    save_skipped_pages(skipped_pages)\n",
    "\n",
    "# Menjalankan fungsi utama dengan menggunakan async\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
